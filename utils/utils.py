import pandas as pd
from sklearn.preprocessing import RobustScaler
from sklearn.neighbors import NearestNeighbors
import os
import streamlit as st
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np


def load_css(css_file):
    """
    Charge et applique un fichier CSS externe pour styliser l'application Streamlit.
    Args:
        css_file (str): Chemin du fichier CSS.
    """
    if os.path.exists(css_file):
        with open(css_file, 'r', encoding='utf-8') as f:
            st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)
    else:
        st.error(f"Fichier CSS non trouv√©: {css_file}")

def load_files(files='films'):
    """
    Charge les fichiers de donn√©es depuis des fichiers CSV.
    Cette fonction permet de charger un ou plusieurs fichiers CSV contenant des donn√©es n√©cessaires √† l'application.
    Elle g√®re les erreurs si le fichier est introuvable ou si le chemin est incorrect.
    Args:
        files (str ou list):
            - Si c'est une cha√Æne de caract√®res (str), elle correspond au nom de base du fichier CSV √† charger.
            - Si c'est une liste (list), elle contient plusieurs noms de base de fichiers CSV √† charger.
    Returns:
        pd.DataFrame ou list:
            - Si un seul fichier est charg√©, retourne un DataFrame Pandas.
            - Si plusieurs fichiers sont charg√©s, retourne une liste de DataFrames Pandas.
    Raises:
        FileNotFoundError: Si un ou plusieurs fichiers sont introuvables.
        ValueError: Si l'argument `files` n'est ni une cha√Æne ni une liste.
    """
    if isinstance(files, str):
        try:
            path = f'csv/{files}_def.csv'
            return pd.read_csv(path)
        except FileNotFoundError:
            raise FileNotFoundError(f"Le fichier '{path}' est introuvable. V√©rifiez le chemin.")
        except Exception as e:
            raise RuntimeError(f"Une erreur inattendue est survenue lors du chargement du fichier '{path}': {e}")
    elif isinstance(files, list):
        dataframes = []
        for file in files:
            try:
                path = f'csv/{file}_def.csv'
                df = pd.read_csv(path)
                dataframes.append(df)
            except FileNotFoundError:
                raise FileNotFoundError(f"Le fichier '{path}' est introuvable. V√©rifiez le chemin.")
            except Exception as e:
                raise RuntimeError(f"Une erreur inattendue est survenue lors du chargement du fichier '{path}': {e}")
        return dataframes
    else:
        raise ValueError("L'argument 'files' doit √™tre une cha√Æne (str) ou une liste de cha√Ænes (list).")

def prepare_features(df):
    """
    Pr√©pare les caract√©ristiques n√©cessaires au syst√®me de recommandation de films.
    √âtapes du traitement :
    - Normalisation des colonnes num√©riques √† l'aide de RobustScaler.
    - Encodage des genres de films sous forme de colonnes binaires.
    Args:
        df (pd.DataFrame): Le DataFrame des films d'origine.
    Returns:
        pd.DataFrame: DataFrame contenant les caract√©ristiques finales pr√™tes pour l'algorithme de recommandation.
    """
    X = df.copy()
    X['year'] = pd.to_datetime(X['release_date']).dt.year  # Extraction de l'ann√©e √† partir de la date de sortie
    numeric_features = ['averageRating', 'popularity', 'year']
    # Normalisation des caract√©ristiques num√©riques
    scaler = RobustScaler()
    X[numeric_features] = scaler.fit_transform(X[numeric_features])
    # Encodage des genres en colonnes binaires
    genres_split = X['genres'].str.split(',').apply(lambda x: [genre.strip() for genre in x])
    genres_dummies = pd.get_dummies(genres_split.explode()).groupby(level=0).sum()
    # Concat√©nation des colonnes num√©riques et des genres encod√©s
    final_features = pd.concat([
        X[numeric_features],
        genres_dummies
    ], axis=1)
    return final_features

def get_recommendations(title, df, features_df, n_recommendations=5, genre_weight=10):
    try:
        movie_index = df[df['title'] == title].index[0]
        genre_columns = [col for col in features_df.columns if col not in ['averageRating', 'popularity', 'year']]
        
        weighted_features = features_df.copy()
        for genre in genre_columns:
            if features_df.iloc[movie_index][genre] == 1:
                weighted_features[genre] = weighted_features[genre] * genre_weight
        
        model = NearestNeighbors(n_neighbors=n_recommendations + 1, metric='euclidean')
        model.fit(weighted_features)
        
        distances, indices = model.kneighbors(weighted_features.iloc[movie_index:movie_index+1])
        
        return df.iloc[indices[0][1:]]
    except Exception as e:
        st.error(f"Erreur dans get_recommendations: {e}")
        return pd.DataFrame()


def format_movie_info(movie):
    try:
        return f"""### üé¨ {movie['title']}
**‚≠ê Note:** {movie['averageRating']}/10
**üìÖ Ann√©e:** {movie['release_date'][:4] if pd.notna(movie['release_date']) else 'Non disponible'}
**üé≠ Genre:** {movie['genres'] if pd.notna(movie['genres']) else 'Non sp√©cifi√©'}

#### üìù Synopsis
{movie.get('overview', 'Synopsis non disponible')}
"""
    except Exception as e:
        st.error(f"Erreur dans format_movie_info: {e}")
        return "Information non disponible"

@st.cache_data(ttl=600)  # –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –Ω–∞ 10 –º–∏–Ω—É—Ç
def search_movies(query, films_df, intervenants_df, lien_df, n_recommendations=5):
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ —Ñ–∏–ª—å–º–æ–≤ –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É –∏–ª–∏ –∏–º–µ–Ω–∏ –∞–∫—Ç–µ—Ä–∞.
    
    Args:
        query (str): –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        films_df (pd.DataFrame): DataFrame —Å —Ñ–∏–ª—å–º–∞–º–∏
        intervenants_df (pd.DataFrame): DataFrame —Å –∞–∫—Ç–µ—Ä–∞–º–∏ –∏ —Ä–µ–∂–∏—Å—Å–µ—Ä–∞–º–∏
        lien_df (pd.DataFrame): DataFrame —Å–≤—è–∑—ã–≤–∞—é—â–∞—è —Ñ–∏–ª—å–º—ã –∏ –ª—é–¥–µ–π
        n_recommendations (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
    
    Returns:
        pd.DataFrame: –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ–∏–ª—å–º—ã
    """
    try:
        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å
        if not query or query.strip() == "":
            return films_df.head(n_recommendations)

        query = query.lower()  # –ü–µ—Ä–µ–≤–æ–¥–∏–º –∑–∞–ø—Ä–æ—Å –≤ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä –æ–¥–∏–Ω —Ä–∞–∑
        
        # –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –ø–æ –∞–∫—Ç—ë—Ä–∞–º - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        actor_matches = intervenants_df[intervenants_df['primaryName'].str.lower().str.contains(query, na=False)]
        
        if not actor_matches.empty:
            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–æ–≥–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –∞–∫—Ç–µ—Ä–∞
            actor_nconst = actor_matches.iloc[0]['nconst']
            
            # –ü–æ–ª—É—á–∞–µ–º —Ñ–∏–ª—å–º—ã —Å —É—á–∞—Å—Ç–∏–µ–º –∞–∫—Ç–µ—Ä–∞
            actor_movies = lien_df[lien_df['nconst'] == actor_nconst]
            matched_films = films_df[films_df['tconst'].isin(actor_movies['tconst'])]
            
            if not matched_films.empty:
                return matched_films.head(n_recommendations)
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å–∫–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞
        # –ò—â–µ–º —Å–Ω–∞—á–∞–ª–∞ —Ç–æ–ª—å–∫–æ –≤ —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø–æ–ª—è—Ö –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        essential_mask = (
            films_df['title'].str.lower().str.contains(query, na=False) |
            films_df['genres'].str.lower().str.contains(query, na=False)
        )
        
        essential_matches = films_df[essential_mask]
        
        if not essential_matches.empty:
            return essential_matches.head(n_recommendations)
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
        extended_mask = (
            films_df['overview'].str.lower().str.contains(query, na=False) |
            films_df['keywords'].str.lower().str.contains(query, na=False) |
            films_df['tagline'].str.lower().str.contains(query, na=False) |
            films_df['origin_country'].str.lower().str.contains(query, na=False)
        )
        
        extended_matches = films_df[extended_mask]
        
        if not extended_matches.empty:
            return extended_matches.head(n_recommendations)
            
        # TF-IDF –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –≤ –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ, –∫–æ–≥–¥–∞ –ø—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π —Ç–µ–∫—Å—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –≤–∞–∂–Ω—ã—Ö –ø–æ–ª–µ–π
        # –∏ —Å –º–µ–Ω—å—à–∏–º —É–º–Ω–æ–∂–µ–Ω–∏–µ–º –¥–ª—è overview
        films_df['search_text'] = (
            films_df['title'].fillna('') + ' ' +
            films_df['overview'].fillna('') + ' ' +
            films_df['keywords'].fillna('') + ' ' +
            films_df['genres'].fillna('')
        )
        
        # –ú–µ–Ω—å—à–µ–µ —á–∏—Å–ª–æ max_features –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        tfidf = TfidfVectorizer(
            stop_words='english',
            max_features=2000,  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 5000
            ngram_range=(1, 1)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ unigrams –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        )
        
        tfidf_matrix = tfidf.fit_transform(films_df['search_text'])
        query_vec = tfidf.transform([query])
        similarity = cosine_similarity(query_vec, tfidf_matrix)
        top_indices = similarity[0].argsort()[-n_recommendations:][::-1]
        
        return films_df.iloc[top_indices]
        
    except Exception as e:
        st.error(f"Erreur dans search_movies: {str(e)}")
        return films_df.head(n_recommendations)  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ñ–∏–ª—å–º—ã –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    
from supabase import create_client, Client

url = "https://ztihcbkzolqvmmiylcnn.supabase.co"
key= "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Inp0aWhjYmt6b2xxdm1taXlsY25uIiwicm9sZSI6ImFub24iLCJpYXQiOjE3MzQ5Njk1NjAsImV4cCI6MjA1MDU0NTU2MH0.z0lQr5IBN6-wMCJROEYLfQOHvn0WVPzsPZut4IWO-Sw"
supabase: Client = create_client(url, key)


def check_authentication():
    """V√©rifie si l'utilisateur est connect√©."""
    return "user" in st.session_state

def handle_signup(email, password):
    """Gestion de l'inscription."""
    if not email or not password:
        st.error("Veuillez remplir tous les champs pour vous inscrire.")
        return
    try:
        response = supabase.auth.sign_up({"email": email, "password": password})
        if response.user:
            user_data = {"id": response.user.id, "email": email}
            try:
                supabase.table("users").insert(user_data).execute()
                st.success("Inscription r√©ussie ! Vous pouvez maintenant vous connecter.")
            except Exception as e:
                st.error(f"Erreur lors de l'ajout √† la base de donn√©es : {str(e)}")
        else:
            st.error("Erreur lors de l'inscription. Veuillez r√©essayer.")
    except Exception as e:
        st.error(f"Une erreur inattendue est survenue : {str(e)}")

def handle_login(email, password):
    """Gestion de la connexion."""
    if not email or not password:
        st.error("Veuillez remplir tous les champs pour vous connecter.")
        return
    try:
        response = supabase.auth.sign_in_with_password({"email": email, "password": password})
        if response.user:
            st.session_state["user"] = {"id": response.user.id, "email": response.user.email}
            st.success(f"Bienvenue {response.user.email} !")
        else:
            st.error("Erreur lors de la connexion. V√©rifiez vos identifiants.")
    except Exception as e:
        st.error(f"Une erreur inattendue est survenue : {str(e)}")

def handle_logout():
    """D√©connexion de l'utilisateur."""
    if "user" in st.session_state:
        del st.session_state["user"]
    st.rerun()